# HLD-02: CAP Theorem

---

## ğŸ“‹ Table of Contents
1. [Introduction](#introduction)
2. [What is CAP Theorem?](#what-is-cap-theorem)
3. [Distributed System with Replicated Data](#distributed-system-with-replicated-data)
4. [The Three Desirable Properties](#the-three-desirable-properties)
   - [C - Consistency](#c---consistency)
   - [A - Availability](#a---availability)
   - [P - Partition Tolerance](#p---partition-tolerance)
5. [The CAP Theorem Rule](#the-cap-theorem-rule)
6. [Why All Three Cannot Be Achieved Together](#why-all-three-cannot-be-achieved-together)
7. [Scenario 1: Trying to Achieve CAP (Not Possible)](#scenario-1-trying-to-achieve-cap-not-possible)
8. [Scenario 2: AP (Availability + Partition Tolerance)](#scenario-2-ap-availability--partition-tolerance)
9. [Scenario 3: CP (Consistency + Partition Tolerance)](#scenario-3-cp-consistency--partition-tolerance)
10. [Scenario 4: CA (Consistency + Availability)](#scenario-4-ca-consistency--availability)
11. [Real-World Trade-offs](#real-world-trade-offs)
12. [Summary](#summary)

---

## Introduction

**CAP Theorem** is a fundamental concept in high-level system design.

**Why is it important?**
- You might be directly asked to define and explain CAP theorem in interviews
- When designing a distributed system, you need to think about CAP **first**
- Your entire design changes based on trade-offs
- If trade-offs are not considered early, it becomes very difficult to change the system design later

**When to consider CAP?**
- In the **very starting phase** of system design
- Before making architectural decisions

---

## What is CAP Theorem?

**CAP Theorem** defines **desirable properties** of a **distributed system** with **replicated data**.

**Remember this definition:**
> CAP Theorem defines desirable properties of a distributed system with replicated data.

---

## Distributed System with Replicated Data

### Example Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Application â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                   â”‚
                â–¼                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  DB Node â”‚        â”‚  DB Node â”‚
         â”‚     B    â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚     C    â”‚
         â”‚ (India)  â”‚  Sync  â”‚   (US)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Characteristics:**
- **Distributed System:** DB nodes spread across different continents
- **Replicated Data:** Data in B is the same as data in C
- **Synchronization:** B and C synchronize data with each other
- Application can query B or C
- User doesn't know which node they're querying

---

## The Three Desirable Properties

**CAP stands for:**
- **C** = Consistency
- **A** = Availability
- **P** = Partition Tolerance

---

## C - Consistency

**Definition:** After a successful write in any node, if you read from any other node, you should get the same data.

### Example

```
Initial State:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚        â”‚  Node C  â”‚
â”‚  A = 4   â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚  A = 4   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Write Operation:
Application writes: A = 5 to Node B

After Write:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚        â”‚  Node C  â”‚
â”‚  A = 5   â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  A = 5   â”‚ (Replicated)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Read Operation:
Application reads from Node C â†’ Should get A = 5
```

**Consistency means:**
- Getting **consistent data** from all nodes
- After successful write on Node B, reading from Node C should return the same value
- All nodes have the **same data** at any point in time

---

## A - Availability

**Definition:** Every request receives a response (whether success or failure). The system is always operational.

### Example

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚              â”‚              â”‚
      â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚   â”‚  Node C  â”‚   â”‚  Node D  â”‚
â”‚ Response â”‚   â”‚ Response â”‚   â”‚ Response â”‚
â”‚ (Success â”‚   â”‚ (Success â”‚   â”‚ (Failure â”‚
â”‚  or Fail)â”‚   â”‚  or Fail)â”‚   â”‚  or Fail)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Availability means:**
- **All nodes should respond**
- Response can be success or failure
- At least they should respond with something:
  - "Data fetched successfully" âœ…
  - "Couldn't fetch the data" âŒ
- As long as all DB nodes are responding â†’ System is **available**

---

## P - Partition Tolerance

**Definition:** When communication between nodes breaks, but the system is still up and able to handle queries.

### Understanding Partition Tolerance

```
Before Partition:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚  Node C  â”‚
â”‚          â”‚  Sync  â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–²                  â–²
      â”‚                  â”‚
      â””â”€â”€â”€â”€â”€â”€ App â”€â”€â”€â”€â”€â”€â”€â”˜

After Partition (Communication Breakage):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•³â•³â•³â•³â•³â•³â•³   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚   Broken   â”‚  Node C  â”‚
â”‚          â”‚            â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–²                      â–²
      â”‚                      â”‚
      â””â”€â”€â”€â”€â”€â”€â”€ App â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         (Still works!)
```

**Key Points:**
- Communication between B and C is **broken**
- B and C cannot replicate data to each other
- **But** the application can still query B or C
- User doesn't know about the internal partition
- User only knows they're querying "the system"
- **Partition has occurred internally** (B is separate, C is separate)
- **System is still up** and responding to queries

**Partition Tolerance means:**
- System continues to operate despite network partitions
- Even if nodes can't communicate with each other, the system doesn't go down

---

## The CAP Theorem Rule

### The Fundamental Rule

```
        Consistency (C)
              â•±  â•²
             â•±    â•²
            â•±      â•²
           â•±   â•³â•³   â•²
          â•±    â•³â•³    â•²
         â•±     â•³â•³     â•²
        â•±      â•³â•³      â•²
       â•±       â•³â•³       â•²
Availability â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Partition
    (A)                 Tolerance (P)
```

**Possible Combinations:**
âœ… **CA** (Consistency + Availability)
âœ… **CP** (Consistency + Partition Tolerance)
âœ… **AP** (Availability + Partition Tolerance)

âŒ **CAP** (All three together) - **NOT POSSIBLE**

**The center area (CAP) is not usable.**

---

## Why All Three Cannot Be Achieved Together

Let's explore all scenarios to understand why CAP is impossible.

---

## Scenario 1: Trying to Achieve CAP (Not Possible)

### Setup

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚        â”‚  Node C  â”‚
â”‚  A = 5   â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚  A = 5   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Requirements
- **C (Consistency):** Data should be the same in both nodes at any time
- **A (Availability):** Both B and C should respond to queries
- **P (Partition Tolerance):** System should stay up even if communication breaks

### What Happens When Partition Occurs

```
Step 1: Partition Happens
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•³â•³â•³â•³â•³â•³â•³   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚   Broken   â”‚  Node C  â”‚
â”‚  A = 5   â”‚            â”‚  A = 5   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: Write Request to Node B
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•³â•³â•³â•³â•³â•³â•³   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚   Broken   â”‚  Node C  â”‚
â”‚  A = 6   â”‚â”€â”€â”€â”€â•³â”€â”€â”€â”€â”€â”€â–ºâ”‚  A = 5   â”‚ (Can't update!)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Updated              Stale Data

Step 3: System State
- Node B has A = 6
- Node C has A = 5
- Communication is broken (partition)
- Both nodes are available (responding)
```

### Result

**Problem:** System became **inconsistent**
- If application queries Node B â†’ Gets A = 6
- If application queries Node C â†’ Gets A = 5 (stale data)

**Conclusion:**
- We achieved **A** (both nodes available)
- We achieved **P** (system still up despite partition)
- We **lost C** (consistency)

**Therefore, CAP together is NOT possible.**

---

## Scenario 2: AP (Availability + Partition Tolerance)

### Trade-off: Drop Consistency

```
Initial State:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚        â”‚  Node C  â”‚
â”‚  A = 5   â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚  A = 5   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Partition Occurs:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•³â•³â•³â•³â•³â•³â•³   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚   Broken   â”‚  Node C  â”‚
â”‚  A = 5   â”‚            â”‚  A = 5   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Write A = 6 to Node B:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•³â•³â•³â•³â•³â•³â•³   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚   Broken   â”‚  Node C  â”‚
â”‚  A = 6   â”‚â”€â”€â”€â”€â•³â”€â”€â”€â”€â”€â”€â–ºâ”‚  A = 5   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  Updated              Inconsistent!
     â–²                      â–²
     â”‚                      â”‚
     â””â”€â”€â”€ Both Available â”€â”€â”€â”˜
```

### What We Achieved

âœ… **Availability (A):**
- Node B responds (success or failure)
- Node C responds (success or failure)
- Both nodes are up and responding

âœ… **Partition Tolerance (P):**
- Communication between B and C is broken
- System is still up
- Application can still query both nodes

âŒ **Consistency (C):**
- Node B has A = 6
- Node C has A = 5
- Data is inconsistent across nodes

### Use Case
- Systems where **availability is more critical** than consistency
- Example: Social media feeds, DNS, caching systems

---

## Scenario 3: CP (Consistency + Partition Tolerance)

### Trade-off: Drop Availability

```
Initial State:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚        â”‚  Node C  â”‚
â”‚  A = 5   â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚  A = 5   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Partition Occurs:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•³â•³â•³â•³â•³â•³â•³   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚   Broken   â”‚  Node C  â”‚
â”‚  A = 5   â”‚            â”‚  A = 5   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

To Maintain Consistency, Take Node C Down:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•³â•³â•³â•³â•³â•³â•³   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚   Broken   â”‚  Node C  â”‚
â”‚  A = 5   â”‚            â”‚   DOWN   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–²                       â•³
     â”‚                       â•³
     â””â”€â”€â”€ Only B Available â”€â”€â•³

Write A = 6 to Node B:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•³â•³â•³â•³â•³â•³â•³   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚   Broken   â”‚  Node C  â”‚
â”‚  A = 6   â”‚            â”‚   DOWN   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  Updated               Not Serving
  Consistent!           Requests
```

### What We Achieved

âœ… **Consistency (C):**
- Only Node B is serving requests
- All queries go to Node B
- Always get consistent data (A = 6)

âœ… **Partition Tolerance (P):**
- Communication is broken
- System is still up (Node B is operational)
- Requests are being fulfilled

âŒ **Availability (A):**
- Node C is down
- Not all nodes are available
- Only one node is responding

### Strategy
- **Take one node down** during partition
- Route all requests to the available node
- Maintain consistency by having single source of truth

### Use Case
- Systems where **consistency is critical**
- Example: Banking systems, financial transactions, inventory management

---

## Scenario 4: CA (Consistency + Availability)

### Trade-off: Drop Partition Tolerance

```
Normal Operation (No Partition):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚        â”‚  Node C  â”‚
â”‚  A = 5   â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚  A = 5   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–²                  â–²
     â”‚                  â”‚
     â””â”€â”€â”€ Both Available & Consistent

If Partition Occurs:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•³â•³â•³â•³â•³â•³â•³   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node B  â”‚   Broken   â”‚  Node C  â”‚
â”‚  A = 5   â”‚            â”‚  A = 5   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â•³                      â•³
     â•³                      â•³
     â””â”€â”€â”€â”€â”€ SYSTEM DOWN â”€â”€â”€â”€â”˜
```

### What We Achieved

âœ… **Consistency (C):**
- When system is up, all nodes have same data
- No inconsistency allowed

âœ… **Availability (A):**
- When system is up, all nodes respond
- Both B and C are available

âŒ **Partition Tolerance (P):**
- If partition happens â†’ System goes down
- Cannot tolerate network failures
- Must stop accepting requests during partition

### Strategy
- **Stop the system** when partition occurs
- Don't allow writes during partition (would cause inconsistency)
- Don't allow reads from inconsistent nodes
- Wait until partition is resolved

### Problem
- If communication breaks for 10 minutes â†’ System is down for 10 minutes
- Not acceptable in modern distributed systems

### Use Case
- Single-node databases (no distribution)
- Systems that can afford downtime
- **Rarely used in modern distributed architectures**

---

## Real-World Trade-offs

### The Golden Rule

**In modern distributed systems:**

âœ… **Never trade off Partition Tolerance (P)**

**Why?**
- Distributed architecture is very common nowadays
- Communication breakage is a **common scenario**
- Network failures happen frequently
- Cannot afford to shut down the entire system for network issues

### The Real Choice

**You must choose between:**
- **CP** (Consistency + Partition Tolerance) â†’ Drop Availability
- **AP** (Availability + Partition Tolerance) â†’ Drop Consistency

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Modern Distributed Systems              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  Always Keep: P (Partition Tolerance)           â”‚
â”‚                                                 â”‚
â”‚  Choose One:                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   OR   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚       CP        â”‚        â”‚       AP        â”‚â”‚
â”‚  â”‚  (Consistency)  â”‚        â”‚ (Availability)  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Interview Question

**Q: What would be your trade-off on Consistency, Availability, and Partition Tolerance?**

**Answer:**
- We will **never trade off Partition Tolerance**
- We will trade off **only between Consistency and Availability**
- Choice depends on the use case:
  - **Banking/Financial systems** â†’ Choose **CP** (Consistency is critical)
  - **Social media/Caching** â†’ Choose **AP** (Availability is critical)

---

## Summary

### Key Takeaways

1. **CAP Theorem Definition:**
   - Defines desirable properties of distributed systems with replicated data
   - C = Consistency, A = Availability, P = Partition Tolerance

2. **The Rule:**
   - Only **2 out of 3** properties can be achieved together
   - **CAP together is impossible**

3. **The Three Properties:**
   - **Consistency:** Same data across all nodes after successful write
   - **Availability:** All nodes respond (success or failure)
   - **Partition Tolerance:** System stays up despite network failures

4. **Possible Combinations:**
   - **AP:** Available + Partition Tolerant (Drop Consistency)
   - **CP:** Consistent + Partition Tolerant (Drop Availability)
   - **CA:** Consistent + Available (Drop Partition Tolerance - rarely used)

5. **Real-World Approach:**
   - Always keep **P** (Partition Tolerance)
   - Choose between **C** and **A** based on requirements

6. **Trade-off Decision:**
   - **Need consistency?** â†’ Choose **CP** (Banking, transactions)
   - **Need availability?** â†’ Choose **AP** (Social media, caching)

### Quick Reference Table

| Combination | What You Get | What You Lose | Use Case |
|-------------|--------------|---------------|----------|
| **AP** | Available + Partition Tolerant | Consistency | Social media, DNS, Caching |
| **CP** | Consistent + Partition Tolerant | Availability | Banking, Financial systems |
| **CA** | Consistent + Available | Partition Tolerance | Single-node DB (rare) |
| **CAP** | All three | Nothing | **IMPOSSIBLE** |

### Important for Interviews

**Remember:**
- CAP is considered in the **starting phase** of system design
- Trade-offs affect the entire architecture
- Partition Tolerance is **non-negotiable** in distributed systems
- The real choice is always between **Consistency** and **Availability**

---

**End of Lecture**

Understanding CAP Theorem is crucial for making architectural decisions in distributed system design. This forms the foundation for designing scalable, reliable systems.
